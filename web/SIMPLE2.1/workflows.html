<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><!-- InstanceBegin template="/Templates/template_new.dwt" codeOutsideHTMLIsLocked="false" -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- InstanceBeginEditable name="doctitle" -->
<title>overview of single-particle cryo-electron microscopy reconstruction with SIMPLE</title>
<!-- InstanceEndEditable -->
<style type="text/css">
body,td,th {
	color: #FFFFFF;
}

body {
	background-image: url(images/nebula.jpg);
	background-attachment: fixed;
	margin-top: 0px;
	margin-right: 0px;
	margin-bottom: 0px;
	margin-left: 0px;
	background-color: rgb(0,0,0);
	background-repeat: repeat;
}


</style>
<link href="common_styles.css" rel="stylesheet" type="text/css">
<script src="SpryAssets/SpryMenuBar.js" type="text/javascript"></script>
<link href="SpryAssets/SpryMenuBarHorizontal.css" rel="stylesheet" type="text/css">
<!-- InstanceBeginEditable name="head" -->
<style type="text/css">
</style>
<script src="SpryAssets/SpryAccordion.js" type="text/javascript"></script>
<link href="SpryAssets/SpryAccordion.css" rel="stylesheet" type="text/css">
<!-- InstanceEndEditable -->
<script type="text/xml">
<!--
<oa:widgets>
  <oa:widget wid="2149022" binding="#OAWidget" />
</oa:widgets>
-->
</script>
<style type="text/css">
a:link {
	color: rgb(153,0,51);
}
a:visited {
	color: rgb(187,0,62);
}
a:hover {
	color: rgb(187,0,62);
}
a:active {
	color: rgb(187,0,62);
}
h1,h2,h3,h4,h5,h6 {
	font-family: Verdana, Geneva, sans-serif;
}
h1 {
	color: rgb(0,0,0);
}
h2 {
	font-size: 18px;
	color: rgb(153,0,0);
}
</style>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31924791-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<link rel="shortcut icon" href="http://simplecryoem.com/favicon.ico" >
</head>

<body bgcolor="#000000">
<a name="top"></a>
<div class="logo_div"><img src="./images/SIMPLE_banner.png" alt="SIMPLE banner" width="830" height="110" align="absmiddle"></div>
<div class="nav_div">
  <ul id="MenuBar1" class="MenuBarHorizontal">
    <li><a href="./index.html">About</a>    </li>
    <li><a href="./download_form.html">Download</a></li>
    <li><a href="./workflows.html">Workflows</a></li>
    <li><a href="./manuals.html">Manuals</a></li>
    <li><a href="./lab.html">Elmlund Lab</a></li>
    <li><a href="./publications.html">Publications</a></li>
    <li><a href="./contact.html">Contact </a></li>
  </ul>
</div>
<!-- InstanceBeginEditable name="EditRegion1" -->
<div class="content_div">
  <h1>SIMPLE 2.1 workflows </h1>
  
  
  
 <!-- Accordion 1 --> 
  <div id="Accordion1" class="Accordion" >
  
  
  
            <div class="AccordionPanel">
          <div class="AccordionPanelTab"><a name="distr"></a>Parallel execution on laptops, workstations and clusters</div>
          <div class="AccordionPanelContent">
          
         <p> On any machine with a single socket (laptop or workstation) it is seldom worth the effort to go beyond the shared-memory parallelisation that we provide using the OpenMP protocol. The shared-memory parallelisation is controlled by the <TT>nthr</TT> key (for number of threads). If your machine has six physical cores and no hyper-threading set <TT>nthr=6</TT> to use all the resources and <TT>nthr=3</TT> to use half the resources. If your machine is hyper-threaded, you may gain performance by increasing the number of threads, depending on the hardware architecture and current workload on the machine. If you have more than one CPU socket on the machine substantial performance enhancements will be gained by executing SIMPLE in distributed mode using the program <FONT COLOR="#0f75ff"><TT>distr_simple.pl</TT></FONT> in the <TT>SIMPLE/scripts</TT> folder. This program fetches machine-specific information that the system administrator (that may be you) is responsible for providing in the <TT>simple_user_input.pm</TT> file. This is how the configuration file looks for a workstation or cluster with any number of sockets, six CPUs per socket and 16GB RAM
<PRE>
    #####################################################################
    # USER-DEFINED VARIABLES THAT CONTROL WHICH CLUSTER ENVIRONMENT TO  #
    # USE AND HOW TO USE THE RESOURCES                                  #
    #####################################################################
    our$SIMPLESYS      = 'LOCAL';          # Name of system
    our%DISTR_ENV      = %LOCAL_DISTR_ENV; # Defines the environment     
    our$EMAIL          = 'myname@uni.edu'; # e-mail for failure report
    our$NTHR           = 6;                # number of threads (CPUs per core)
    our$MEMSTR         = '8000';           # string descriptor for memory
    our$TIME_PER_IMAGE = 150;              # time per image (in seconds)
</PRE>
<p>If we had eight CPUs per socket and 32GB RAM we would have changed the number of threads to <TT>$NTHR=8</TT> and the requested memory to <TT>$MEMSTR=16000</TT>. We will describe what the <TT>$SIMPLESYS</TT> and <TT>%DISTR_ENV</TT> variables control after we have discussed how to optimise distributed execution of SIMPLE on any heterogeneous computer cluster (Figure 1, below).  </p>
<p><img src="images/cputopo4web.png" width="799" height="513" alt=""/><BR>
  
  <BR>
  Every cluster is equipped with a job scheduler/workload manager that needs to be configured. The two most common job schedulers are SLURM (Simple Linux Utility for Resource Management) and PBS (Portable Batch System). We prefer SLURM, since it is a more modern and versatile job scheduler than PBS. All the instructions that need to be provided to the job scheduler have been separated out and put in the perl configuration module <TT>scripts/simple_clusterSpecs.pm</TT>. A typical SLURM configuration is defined as</p>

<p>&nbsp;</p>
<PRE>
    ####################################################################
    # DEFINES DISTRIBUTED EXECUTION ON MYCLUSTER                       #
    ####################################################################
    our%MYCLUSTER_DISTR_ENV;
    $MYCLUSTER_DISTR_ENV{'SUBMITCMD'}='sbatch';
    $MYCLUSTER_DISTR_ENV{'SCRIPT'}="#!/bin/bash
    #SBATCH --mail-user=&lt;&lt;&lt;&lt;EMAIL&gt;&gt;&gt;&gt;
    #SBATCH --mail-type=FAIL
    #SBATCH --job-name=$NAME_DISTR
    #SBATCH --ntasks=1
    #SBATCH --ntasks-per-socket=1
    #SBATCH --cpus-per-task=&lt;&lt;&lt;NTHR&gt;&gt;&gt;
    #SBATCH --mem=&lt;&lt;&lt;MEMSTR&gt;&gt;&gt;
    #SBATCH --time=0-&lt;&lt;&lt;HOURS&gt;&gt;&gt;:&lt;&lt;&lt;MINUTES&gt;&gt;&gt;:0
    #SBATCH --output=outfile.%j
    #SBATCH --error=errfile.%j
    cd &lt;&lt;&lt;EXECDIR&gt;&gt;&gt; 
    &lt;&lt;&lt;CMDSTRING&gt;&gt;&gt; fromp=&lt;&lt;&lt;START&gt;&gt;&gt; top=&lt;&lt;&lt;STOP&gt;&gt;&gt; part=&lt;&lt;&lt;PART&gt;&gt;&gt;&amp;
    &amp;nthr=&lt;&lt;&lt;NTHR&gt;&gt;&gt; outfile=$ALGNDOC_FBODY&lt;&lt;&lt;PART&gt;&gt;&gt;.txt &gt; OUT&lt;&lt;&lt;PART&gt;&gt;&gt;\nexit\n";
    $MYCLUSTER_DISTR_ENV{'SHMEMSCRIPT'}="#!/bin/bash
    #SBATCH --mail-user=&lt;&lt;&lt;&lt;EMAIL&gt;&gt;&gt;&gt;
    #SBATCH --mail-type=FAIL
    #SBATCH --job-name=$NAME_SHMEM_DISTR
    #SBATCH --ntasks=1
    #SBATCH --ntasks-per-socket=1
    #SBATCH --cpus-per-task=&lt;&lt;&lt;NTHR&gt;&gt;&gt;
    #SBATCH --mem=&lt;&lt;&lt;MEMSTR&gt;&gt;&gt;
    #SBATCH --time=0-&lt;&lt;&lt;HOURS&gt;&gt;&gt;:&lt;&lt;&lt;MINUTES&gt;&gt;&gt;:0
    #SBATCH --output=shmemoutfile.%j
    #SBATCH --error=shmemerrfile.%j
    cd &lt;&lt;&lt;EXECDIR&gt;&gt;&gt; 
    &lt;&lt;&lt;CMDSTRING&gt;&gt;&gt; nthr=&lt;&lt;&lt;NTHR&gt;&gt;&gt; &gt; SHMEMJOBOUT\nexit\n";
</PRE>
The <TT>&amp;</TT> character denotes a line break and the substitution tag
<PRE>
    &lt;&lt;&lt;MYVARIABLE&gt;&gt;&gt;
</PRE>
describes variables that will be automatically substituted into the scripts. The variables <TT>$NAME_DISTR</TT> and <TT>$NAME_SHMEM_DISTR</TT> are local to the module and describe the hardcoded names of the distribution scripts. In order to make the newly defined distributed environment accessible to SIMPLE we need to export it by adding it to the export array in the header of the module, so that the line
<PRE>
    @EXPORT = qw($ALGNDOC_FBODY %LOCAL_DISTR_ENV %MASSIVE_DISTR_ENV&amp;
    &amp;%MASSIVE2_DISTR_ENV %MONARCH_DISTR_ENV %OXFORD_DISTR_ENV&amp;
    &amp;%OXFORD2_DISTR_ENV %OXFORD3_DISTR_ENV  $CVL_DISTR_ENV);
</PRE>
is updated to
<PRE>
    @EXPORT = qw($ALGNDOC_FBODY %LOCAL_DISTR_ENV %MASSIVE_DISTR_ENV&amp;
    &amp;%MASSIVE2_DISTR_ENV %MONARCH_DISTR_ENV %OXFORD_DISTR_ENV&amp;
    &amp;%OXFORD2_DISTR_ENV %OXFORD3_DISTR_ENV  $CVL_DISTR_ENV)&amp;
    &amp;%MYCLUSTER_DISTR_ENV;
</PRE>
There are different versions of SLURM and PBS and different clusters may use different conventions for how to construct the script headers. For example on our <TT>MASSIVE2</TT> cluster we need to add to the headers
<PRE>
    #SBATCH --partition=cryoem
    #SBATCH --qos=vip_m2
</PRE>
to indicate that we will use our dedicated <TT>cryoem</TT> partition and our dedicated queue <TT>vip_m2</TT>. In <TT>simple_clusterSpecs.pm</TT> there is also a template available for PBS
<PRE>
    ####################################################################
    # DEFINES DISTRIBUTED EXECUTION ON THE MASSIVE 1 CLUSTER           #
    ####################################################################
    our%MASSIVE_DISTR_ENV;
    $MASSIVE_DISTR_ENV{'SUBMITCMD'}='qsub';
    $MASSIVE_DISTR_ENV{'SCRIPT'}="#!/bin/bash
    #PBS -N $NAME_DISTR
    #PBS -l nodes=1:ppn=&lt;&lt;&lt;NTHR&gt;&gt;&gt;,mem=&lt;&lt;&lt;MEMSTR&gt;&gt;&gt;
    #PBS -l walltime=&lt;&lt;&lt;HOURS&gt;&gt;&gt;:&lt;&lt;&lt;MINUTES&gt;&gt;&gt;:0
    #PBS -o outfile.\$PBS_JOBID
    #PBS -e errfile.\$PBS_JOBID
    cd &lt;&lt;&lt;EXECDIR&gt;&gt;&gt; 
    &lt;&lt;&lt;CMDSTRING&gt;&gt;&gt; fromp=&lt;&lt;&lt;START&gt;&gt;&gt; top=&lt;&lt;&lt;STOP&gt;&gt;&gt; part=&lt;&lt;&lt;PART&gt;&gt;&gt;&amp;
    &amp;nthr=&lt;&lt;&lt;NTHR&gt;&gt;&gt; outfile=$ALGNDOC_FBODY&lt;&lt;&lt;PART&gt;&gt;&gt;.txt &gt; OUT&lt;&lt;&lt;PART&gt;&gt;&gt;\nexit\n";
    $MASSIVE_DISTR_ENV{'SHMEMSCRIPT'}="#!/bin/bash
    #PBS -N $NAME_SHMEM_DISTR
    #PBS -l nodes=1:ppn=&lt;&lt;&lt;NTHR&gt;&gt;&gt;,mem=&lt;&lt;&lt;MEMSTR&gt;&gt;&gt;
    #PBS -l walltime=&lt;&lt;&lt;HOURS&gt;&gt;&gt;:&lt;&lt;&lt;MINUTES&gt;&gt;&gt;:0
    #PBS -o outfile.\$PBS_JOBID
    #PBS -e errfile.\$PBS_JOBID
    cd &lt;&lt;&lt;EXECDIR&gt;&gt;&gt; 
    &lt;&lt;&lt;CMDSTRING&gt;&gt;&gt; nthr=&lt;&lt;&lt;NTHR&gt;&gt;&gt; &gt; SHMEMJOBOUT\nexit\n";
</PRE>
but PBS does not provide any means to bind a set of threads to a particular socket. However, by utilising the <TT>mpirun</TT> command we can enforce this desired behaviour as exemplified below
<PRE>
    ####################################################################
    # DEFINES DISTRIBUTED EXECUTION ON SUSANS CLUSTER IN OXFORD        #
    ####################################################################
    our%OXFORD_DISTR_ENV;
    $OXFORD_DISTR_ENV{'SUBMITCMD'}='qsub';
    $OXFORD_DISTR_ENV{'SCRIPT'}="#!/bin/bash
    #PBS -N $NAME_DISTR
    #PBS -l nodes=1:ppn=&lt;&lt;&lt;NTHR&gt;&gt;&gt;,mem=&lt;&lt;&lt;MEMSTR&gt;&gt;&gt;
    #PBS -l walltime=&lt;&lt;&lt;HOURS&gt;&gt;&gt;:&lt;&lt;&lt;MINUTES&gt;&gt;&gt;:0
    #PBS -o outfile.\$PBS_JOBID
    #PBS -e errfile.\$PBS_JOBID
    #PBS -V
    #PBS -l naccesspolicy=UNIQUEUSER
    cd &lt;&lt;&lt;EXECDIR&gt;&gt;&gt; 
    mpirun -np 1 --bind-to-socket --cpus-per-proc &lt;&lt;&lt;NTHR&gt;&gt;&gt; &lt;&lt;&lt;CMDSTRING&gt;&gt;&gt;&amp;
    &amp;fromp=&lt;&lt;&lt;START&gt;&gt;&gt; top=&lt;&lt;&lt;STOP&gt;&gt;&gt; part=&lt;&lt;&lt;PART&gt;&gt;&gt; nthr=&lt;&lt;&lt;NTHR&gt;&gt;&gt;&amp;
    &amp;outfile=$ALGNDOC_FBODY&lt;&lt;&lt;PART&gt;&gt;&gt;.txt &gt; OUT&lt;&lt;&lt;PART&gt;&gt;&gt;\nexit\n";
    $OXFORD_DISTR_ENV{'SHMEMSCRIPT'}="#!/bin/bash
    #PBS -N $NAME_SHMEM_DISTR
    #PBS -l nodes=1:ppn=&lt;&lt;&lt;NTHR&gt;&gt;&gt;,mem=&lt;&lt;&lt;MEMSTR&gt;&gt;&gt;
    #PBS -l walltime=&lt;&lt;&lt;HOURS&gt;&gt;&gt;:&lt;&lt;&lt;MINUTES&gt;&gt;&gt;:0
    #PBS -o outfile.\$PBS_JOBID
    #PBS -e errfile.\$PBS_JOBID
    #PBS -V
    #PBS -l naccesspolicy=UNIQUEUSER
    cd &lt;&lt;&lt;EXECDIR&gt;&gt;&gt; 
    mpirun -np 1 --bind-to-socket --cpus-per-proc &lt;&lt;&lt;NTHR&gt;&gt;&gt; &lt;&lt;&lt;CMDSTRING&gt;&gt;&gt;&amp;
    &amp;nthr=&lt;&lt;&lt;NTHR&gt;&gt;&gt; &gt; SHMEMJOBOUT\nexit\n";
</PRE>
Once our environment for distributed execution is established we use the Program: <FONT COLOR="#0f75ff"><TT>distr_simple.pl</TT></FONT>, which supports distributed execution of the programs:
<PRE>
    simple_prime2D
    simple_prime3D
    simple_eo_recvol
    simple_recvol
    simple_simemimgs
</PRE>
We normally let <FONT COLOR="#0f75ff"><TT>distr_simple.pl</TT></FONT> run in the background on the login node of our cluster. We will discuss the execution routes in more detail in the <TT>Workflows</TT> section but an example of how to distribute <FONT COLOR="#0f75ff"><TT>simple_prime2D</TT></FONT> using ten nodes is provided below. In order to reduce I/O latency we split the CTF phase-flipped image stack into as many partitions (<TT>npart</TT>) as we plan to execute
<PRE>
     $ simple_stackops stk=my_phaseflipped_ptcls.mrc split=npart
</PRE>
Then, we are ready to execute in distributed mode
<PRE>
     $ nohup distr_simple.pl prg=prime2D npart=10 stk=ptcls.mrc smpd=1.77 msk=100
     ncls=600 refs=startcavgsmsk.mrc oritab=prime2D_startdoc.txt &gt;&gt; PRIME2DOUT &amp;
</PRE>
Another option available on clusters that use the SLURM scheduler is to use the <TT>srun</TT> command for <FONT COLOR="#0f75ff"><TT>distr_simple.pl</TT></FONT> via
<PRE>
    $ srun --ntasks=1 --ntasks-per-socket=1 --cpus-per-task=1 --mem=200 --time=2-0:0:0
    --output=PRIME2DOUT.%j --error=PRIME2DERR.%j distr_simple.pl prg=prime2D 
    npart=10 stk=ptcls.mrc smpd=1.77 msk=100 ncls=600 refs=startcavgsmsk.mrc 
    oritab=prime2D_startdoc.txt &amp;
</PRE>
However, beta testers have reported that srun job sometimes dies with no warning, possibly because of the low tolerance for network errors. A more robust route may be to use <TT>sbatch</TT> as follows
<PRE>
    $ sbatch -p MYCLUSTER --wrap="distr_simple.pl prg=prime2D npart=10 stk=ptcls.mrc
    smpd=1.77 msk=100 ncls=600 refs=startcavgsmsk.mrc oritab=prime2D_startdoc.txt 
    &gt;&gt; PRIME2DOUT"
</PRE>
where the <TT>-wrap</TT> flag automatically generates a bash script for the given command. </p>
   
          </div>
              </div>  
              
              
              
              <div class="AccordionPanel">
      <div class="AccordionPanelTab">CTF phase flipping</div>
      <div class="AccordionPanelContent">
        <p><strong>SIMPLE</strong> uses the same CTF convention as <a href="http://grigoriefflab.janelia.org/ctf" target="new">CTFFIND</a> with the exception that defocus values are inputted in microns rather than Angstroms. The astigmatism angles are in units of degree. If you have a particle stack of uncorrected windowed single-particle images and you wish to multiply them with the sign of the CTF (phase flipping), please create a text file looking like
<PRE>
    dfx=2.56 dfy=2.76 angast=30.5
    dfx=3.50 dfy=3.33 angast=60.0
    dfx=1.98 dfy=2.02 angast=120.5
    ...
</PRE>
with the same number of lines as the number of images in the stack, so that there is a one-to-one correspondence between each line of CTF parameters in the text file and each particle image in the stack. Now, use the program <a href="manuals.html#operations" target="new">simple_stkops</a> to phase flip the stack
<PRE>
    $ simple_stackops stk=ptcls.mrc smpd=2 deftab=ctfparams.txt 
    ctf=flip kv=300 cs=2.7 fraca=0.07 outstk=ptcls_phflip.mrc
</PRE>
and use the corrected stack <TT>ptcls_phflip.mrc</TT> as input for the remaining workflows. <br> Save the <TT>ctfparams.txt</TT> file somewhere safe, you might need it for future Wiener restoration in the refinement.</p>
    
        </div>
    </div>
              
              
              
              
            <div class="AccordionPanel">
          <div class="AccordionPanelTab">2D alignment/clustering with PRIME2D</div>
          <div class="AccordionPanelContent">
          
         <p> <img src="images/prime2D_wflows4web.png" alt="" width="400" height="759" class="tfiid_movie"/>We provide a solver for the problem of simultaneous alignment and clustering of cryo-EM images (SAC) implemented in the program <a href="manuals.html#prime2D" target="new">simple_prime2D</a>. It is assumed that you have a SPIDER or MRC stack of phase flipped particle images (see above). The flowcharts of the workflows involving <a href="manuals.html#prime2D" target="new">simple_prime2D</a> are depicted in the flowchart. <br>

There are two modes of execution: plain "2D alignment/clustering", assuming that you have a clean and nice data set with not too much junk, such as ice contamination or particle aggregation. You begin executing <a href="manuals.html#prime2D" target="new">simple_prime2D_init</a> to produce the files</p>
         <p> <TT>startcavgsmsk.mrc</TT></p>
         <p><TT>prime2D_startdoc.txt</TT> </p>
         <p>containing the random references and random clustering solution. These files are are next used as input to <a href="manuals.html#prime2D" target="new">simple_prime2D</a>. However, if you are planning on executing SIMPLE on a multi-socket workstation or cluster using <FONT COLOR="#0f75ff"><TT>distr_simple.pl</TT></FONT> you have to split the stack into as many partitions (nodes) you are planning to run your job on. This step is necessary for reducing I/O latency. On single-socket machines it is not necessary to split the stack. <br><br> The files generated from <a href="manuals.html#prime2D" target="new">simple_prime2D_init</a> are inputted into <a href="manuals.html#prime2D" target="new">simple_prime2D</a> together with the particle stack and a few control parameters, such as sampling distance (<TT>smpd</TT>), mask radius in pixels (<TT>msk</TT>), number of desired clusters (<TT>ncls</TT>) and low-pass limit (<TT>lp</TT>). The default low-pass limit is set to <TT>lp=20 &#197;</TT> which is suitable for all particles with a molecular weight above 300 kDa. You may have to include higher frequency components to obtain a good clustering solution for smaller molecules but beware of the problem of overfitting. If too much high frequency information is included in the search, the solution obtained may be dominated by noise.<br><br>
           We also provide a "cleanup" workflow for the processing of more challenging data sets. Perhaps your particles were automatically boxed and the stack includes a lot of false positives, such as ice contamination, particle aggregation, hole edges etc. The first part of the "cleanup" workflow is identical to the original workflow. Next, the final class averages obtained with the first pass of clustering are ranked according to decreasing population with <a href="manuals.html#prime2D" target="new">simple_rank_cavgs</a> and a GUI (we use <a href="http://blake.bcm.edu/emanwiki/EMAN2" target="new">EMAN</a>) is used to remove unwanted class averages. The program <a href="manuals.html#prime2D" target="new">simple_map2ptcls</a> is then applied to map your manual selection of class averages back to the particle images. The selection is communicated via a text document named <TT>mapped_ptcls_params.txt</TT> by default. Every particle image receives a state assignment of one by default (<TT>state=1</TT>) and the particles corresponding to deleted class averages are assigned a state label of zero (<TT>state=0</TT>) in the outputted document. This prevents them from being considered in future processing steps. In order to obtain a "clean" clustering solution, execute the original workflow again but now inputting the <TT>mapped_ptcls_params.txt</TT> document to the <a href="manuals.html#prime2D" target="new">simple_prime2D_init</a> initialiser in order to propagate the selection. </p>
          
          
          </div>
              </div>  



              <div class="AccordionPanel">
          <div class="AccordionPanelTab">3D reconstruction with PRIME3D</div>
          <div class="AccordionPanelContent">
          
          <p><img src="images/abinitio_wflow4web.png" alt="" width="400" height="262" class="tfiid_movie"/></p>
          <p>We provide an <I>ab initio </I> 3D reconstruction algorithm that operates either on class averages or on individual particle images. For small data sets (&lt;10,000) images it is usually a better idea to skip the 2D alignment/clustering step and go directly to PRIME3D. For larger data sets or data sets with a lot of contamination, such as ice, particle aggregation etc. we advise running PRIME2D first in "cleanup" mode and calculating an <I>ab initio</I> map from "clean" class averages. The 3D PRIME algorithm is implemented in the program <a href="manuals.html#prime3D" target="new">simple_prime3D</a>. <br><br>
          It is assumed that you have a SPIDER or MRC stack of class averages or phase flipped particle images. The flowchart for the PRIME3D <I>ab initio</I> reconstruction workflow is depicted in Figure 3. You begin executing <a href="manuals.html#prime3D" target="new">simple_prime3D_init</a> to produce the files <TT>startvol_state1.mrc</TT> and <TT>prime3D_startdoc.txt</TT> containing the initial random reference and the orientations used to obtain it. These files are then used as input to <a href="manuals.html#prime3D" target="new">simple_prime3D</a>. <br><br>
          However, if you are planning on executing SIMPLE on a multi-socket workstation or cluster using <FONT COLOR="#0f75ff"><TT>distr_simple.pl</TT></FONT> you have to split the stack into as many partitions (nodes) you are planning to run your job on. This step is necessary for reducing I/O latency. On single-socket machines it is not necessary to split the stack. Next, the generated files are inputted to <a href="manuals.html#prime3D" target="new">simple_prime3D</a> together with the particle stack and a few control parameters, such as sampling distance (<TT>smpd</TT>) and mask radius in pixels (<TT>msk</TT>). <br><br>
          
          Details about how to run PRIME3D and how the initial low-pass limit is set and how it is updated throughout a PRIME3D run are described in the <a href="manuals.html#prime3D" target="new">simple_prime3D</a> section. To check the automatically determined low-pass limit range, use the <a href="manuals.html#prime3D" target="new">simple_resrange</a> program.</p>
          
          </div>
              </div>  
              
              
              
              
              
              
              
              
              
              
              <div class="AccordionPanel">
          <div class="AccordionPanelTab">Worked-out example on GroEL</div>
          <div class="AccordionPanelContent">
          
          <P>
To further illustrate how to use the SIMPLE suite of programs, we provide the following comprehensive worked-out example, including all commands executed when reconstructing the D7 symmetric GroEL chaperonin. The same workflow was used to process a series of experimental datasets with high, low or no symmetry, described in our recent <a href="http://dx.doi.org/10.1016/j.str.2016.04.006" target="new">paper</a>. The workflow consists of four major steps

<OL>
<LI>2D alignment and clustering of the images using <a href="manuals.html#prime2D" target="new">simple_prime2D</a>
</LI>
<LI>asymmetric 3D reconstruction from the class averages using <a href="manuals.html#prime3D" target="new">simple_prime3D</a>
</LI>
<LI>symmetrisation of the volume using <a href="manuals.html#prime3D" target="new">simple_symsearch</a>
</LI>
<LI>refinement of the symmetrised volume using <a href="manuals.html#prime3D" target="new">simple_prime3D</a>
</LI>
</OL>

The data set consisted of 10,000 phase flipped images with 140x140 pixels dimension, randomly selected from a larger publicly available data set XXXXXX. Throughout the different steps of the workflow we used a circular mask radius of <TT>msk=60</TT> pixels and sampling distance of <TT>smpd=1.62</TT>.
 
 <H3><A NAME="SECTION00074100000000000000"></A>
 <A NAME="2dclust"></A>
 <BR>
 2D alignment and clustering of the images
 </H3>

 <P>
 Prior to 2D alignment and clustering, we begin by minimising the effect that off-centre particles could have on the subsequent steps. The method is not aimed at determining the rotational origin shifts exactly but only to roughly centre the particles in the box. This is done by bringing all particle images into broad register with respect to their 2D shifts only, regardless of their in-plane rotation. <br><br>
 
 <a href="manuals.html#operations" target="new">simple_stkops</a> with the argument <TT>shalgn=yes</TT> is used, providing our stack (<TT>stk=particles.spi</TT>), sampling distance (<TT>smpd=1.62</TT>) and mask radius in pixels (<TT>msk=60</TT>) as input
 <PRE>
     $ simple_stackops stk=particles.spi smpd=1.62 msk=60 
     shalgn=yes trs=3.5 lp=20 nthr=8 outstk=particles_sh.spi
 </PRE>
 The shift alignment is done with a hard low-pass limit of 20 &#197; (<TT>lp=20</TT>), as are most of the following steps. The iterative process will typically take a dozen iterations (a few minutes). The <TT>trs</TT> argument limits the shift search to the XXXXX
  range. We typically set the <TT>trs</TT> argument to 2.5% of the image dimension (140). There are 8 CPUs on our machine so we set the number of threads <TT>nthr=8</TT>. A new centred stack (named according to the <TT>outstk</TT> argument) will be written to disk and we will use this one for the remainder of the workflow. A document named <TT>shiftdoc.txt</TT> by default that contains the calculated shifts is also created.

 <P>
 Next we generate random class averages to initiate the 2D clustering procedure. Given the modest size of our dataset (10,000 images) we choose <TT>ncls=200</TT> to obtain sufficiently populated classes. We recommended increasing this number to at least 500 for larger datasets (&gt;30,000 images).
 <PRE>
     $ simple_prime2D_init stk=particles_sh.spi smpd=1.62 msk=60 ncls=200 nthr=8
 </PRE>
 <a href="manuals.html#prime2D" target="new">simple_prime2D_init</a> will rapidly generate evenly populated class averages with random in-plane rotations. The stacks of 200 class averages are named <TT>startcavgsmsk.spi</TT> and <TT>startcavgs.spi</TT> (with and without mask). Next, we execute the 2D alignment and clustering in distributed mode
 <PRE>
     $ simple_stackops stk=particles_sh.spi split=1
     $ nohup distr_simple.pl prg=prime2D stk=particles_sh.spi
     oritab=prime2D_startdoc.txt refs=startcavgsmsk.spi ncls=200
     srch_inpl=yes smpd=1.62 msk=60 lp=20 npart=1 &gt; PRIME2DOUT &amp;
 </PRE>
 The first instruction prepares the split stack for distributed execution. In our case we ran the clustering on a Linux workstation with 1 CPU chipset so we simply set <TT>split=1</TT>. If your machine has two chipsets, set split to 2 but keep in mind that the <TT>npart</TT> argument in the following instruction also needs to be set to 2. The second instruction starts the actual 2D clustering using the randomised classes as a starting point (<TT>refs</TT> argument). It will take approximately 15 iterations and a little under 2 hours on a modern workstation with 8 CPUs. In the last lines of the log file <TT>PRIME2DOUT</TT> you should see something looking like
 <PRE>
     &gt;&gt;&gt; DISTRIBUTION OVERLAP:                 0.9589
     &gt;&gt;&gt; PERCENTAGE OF SEARCH SPACE SCANNED:  99.6
     &gt;&gt;&gt; CORRELATION:                          0.7521
     &gt;&gt;&gt; CONVERGED: .YES.
 </PRE>
 Our criterion for convergence is based the stability of the clusters obtained. In other words, when the cluster assignments are nearly identical from one iteration to the next (distribution overlap  &gt;95% on average) and the particles cannot find a better matching average (fraction of search space scanned &gt;99%) the alignment and clustering stops. In addition, each run is structured as follows. Until near convergence (search space scanned &lt;90%) only cluster assignment and in-plane rotations are searched. After this, shifts are also searched and their limit is automatically set to 2.5% of the image dimension (see above). Every iteration produces a folder named <TT>prime2D_round_XX</TT> that contains all the information to continue a run: a document with the current in-plane parameters (<TT>prime2Ddoc_XX.txt</TT>) and two stacks of the current 200 class averages (masked and unmasked).

 <P>
 A number of temporary files are also created but they are only used internally and will be automatically deleted at the end of the run. As computer and network failures are part of using workstations and supercomputers you will be able to continue an interrupted run using the files present in these self-contained folders. You can also automatically remove the temporary files by simply typing <FONT COLOR="#0f75ff"><TT>prime_cleanup.pl</TT></FONT>. Never do this while the application is running.  It is also necessary to keep the current folder organised to avoid data loss and confusion. We do not need the split stack anymore, so type
 <PRE>
     $ rm stack_part*.spi
 </PRE>
 Visual examination of the 200 class averages (<TT>prime2D_round_15/cavgs_iter15.spi</TT>) shows numerous images with distinctive features of GroEL such as the double ring structure and the heptameric C-symmetric rings on a uniform grey background. One can also note blurrier images with less contrast. Typically, these correspond to lowly populated classes where the weaker SNR is likely to contribute little to the subsequent 3D reconstruction. Consequently, we rank the class averages by decreasing order of their population
 <PRE>
     $ simple_rank_cavgs stk=prime2D_round_15/cavgs_iter15.spi 
     oritab=prime2D_round_15/prime2Ddoc_15.txt outstk=ranked_cavgs.spi
 </PRE>
 After visual inspection of the ranked class averages (we use <a href="http://blake.bcm.edu/emanwiki/EMAN2">EMAN</a> for this) we decide to discard the noisier/blurrier images by keeping the first 160 averages in the ranked stack. This discards clusters containing less than 30 images per class. We simply extract the top 160 averages using the command
 <PRE>
     $ simple_stackops stk=ranked_cavgs.spi fromp=1 top=160 outstk=selected_cavgs.spi
 </PRE>
 where <TT>fromp</TT> and <TT>top</TT> define the range of images to keep. With this reduced stack (<TT>selected_cavgs.spi</TT>) we will generate an <I>ab initio</I> 3D reconstruction of the molecule using <a href="manuals.html#prime3D" target="new">simple_prime3D</a>
 
 
 
 <H3><A NAME="SECTION00074200000000000000">
 <I>Ab initio</I> 3D reconstruction</A>
 </H3>

 <P>
 We first need a random volume to initiate the search of the five in-plane and out-of-plane parameters of our selected class averages. As in the previous 2D analysis, we execute
 <PRE>
     $ simple_prime3D_init stk=selected_cavgs.spi smpd=1.62 msk=60 nthr=8 lp=20
 </PRE>
 Consistently with the previous section we use a low-pass limit of 20 &#197; (<TT>lp=20</TT>). This command will generate two files: a volume reconstructed from random orientation parameters (<TT>startvol_state1.spi</TT>) and the document containing these parameters (<TT>prime3D_startdoc.txt</TT>). We start the search with
 <PRE>
     $ simple_stackops stk=selected_cavgs.spi split=1
     $ nohup distr_simple.pl prg=prime3D stk=selected_cavgs.spi 
     vol1=startvol_state1.spi smpd=1.62 msk=60 lp=20 
     oritab=prime3D_startdoc.txt npart=1 &gt; PRIME3DOUT &amp;
 </PRE>
 Again, we first split the stack for distributed execution. Then, we run <a href="manuals.html#prime3D" target="new">simple_prime3D</a> providing the randomised orientations (<TT>oritab</TT> argument) and volume (<TT>vol1</TT> argument) that we have just prepared. After approximately 16 iterations the run converges. At the end of PRIME3DOUT you will find
 <PRE>
     &gt;&gt;&gt; ANGLE OF FEASIBLE REGION:            14.1
     &gt;&gt;&gt; AVERAGE ANGULAR DISTANCE BTW ORIS:    2.4 
     &gt;&gt;&gt; PERCENTAGE OF SEARCH SPACE SCANNED: 100.0
     &gt;&gt;&gt; CORRELATION:                          0.9178
     &gt;&gt;&gt; ANGULAR SDEV OF MODEL:               40.81
     &gt;&gt;&gt; UPDATE LOW-PASS LIMIT: .NO.
     &gt;&gt;&gt; CONVERGED: .YES.
 </PRE>
 The <TT>recvol_state16.spi</TT> volume and the corresponding orientation parameters (<TT>prime3Ddoc_16.txt</TT>) are produced in the <TT>prime3D_round_16</TT> folder. The volume is blobby but still captures the overall shape of GroEL. Keep in mind that we have so far made no assumption about symmetry and the volume has been reconstructed in the C1 symmetry group.
 
 <H3><A NAME="SECTION00074300000000000000">
 Symmetrisation of the volume</A>
 </H3>

 <P>
 In order to symmetrise the volume, we need to identify the principal axis of symmetry given the known D7 point-group symmetry group of GroEL. This is done with <a href="manuals.html#prime3D" target="new">simple_symsearch</a>, given the C1 volume and orientation parameters (<TT>vol1</TT> and <TT>oritab</TT>) and using the same low-pass limit as previously (<TT>lp=20</TT>). The symmetrised orientations are outputted in the text file <TT>sym_d7.txt</TT> (<TT>outfile</TT> argument). We identify the principal symmetry axis of the volume by executing 
 <PRE>
     $ simple_symsrch vol1=prime3D_round_16/recvol_state1.spi smpd=1.62 msk=60 
     oritab=prime3D_round_16/prime3Ddoc_16.txt pgrp=d7 outfile=sym_d7.txt nthr=8 
     lp=20 &gt; SYMOUT
 </PRE>
 The program prints the identified symmetry axis
 <PRE>
     &gt;&gt;&gt; FOUND SYMMETRY AXIS ORIENTATION:
     e1=276.596588 e2=80.9958649 e3=297.840454 x=0.00000000 y=0.00000000 
     mi=0.00000000 mi_hard=0.00000000 dist=180.000000 state=1.00000000 
     corr=0.787599325 w=1.00000000 class=1.00000000 mirr=0.00000000 frac=0.00000000
 </PRE>
 and we use <a href="manuals.html#prime3D" target="new">simple_eo_recvol</a> to reconstruct a symmetrised volume
 <PRE>
     $ simple_eo_recvol stk=selected_cavgs.spi 
     oritab=sym_d7.txt smpd=1.62 msk=60 nthr=8 pgrp=d7
 </PRE>
 Output is the optimal principal axis of symmetry (<TT>e1</TT>, <TT>e2</TT> and <TT>e3</TT> are the phi, theta and psi angles) along with its correlation (<TT>corr</TT>). It is likely that you will obtain different values for the axis of symmetry upon different runs. This is because of the stochastic nature of the 2D/3D analyses, which causes the 3D reconstruction to be arbitrarily oriented with respect to the principal symmetry axis. Nonetheless, the final volume is reproducible and captures the structure of GroEL as judged by the numerous existing crystallographic and EM structures.

 <P>
 With the new symmetrised orientation parameters of the class averages we now want to map this information back to the individual particle images. To do this we first create a text file called <TT>doclist.txt</TT> that contains a single line
 <PRE>
     $ ls -1 sym_d7.txt &gt; doclist.txt
 </PRE>
 and then map the orientation parameters of the class averages to the particles (<TT>stk</TT>) by providing the selected class averages (<TT>stk2</TT>), all the original class averages (<TT>stk3</TT>) and the in-plane parameters obtained in the first <a href="manuals.html#prime2D" target="new">simple_prime2D</a> run (<TT>oritab</TT>) to the program <a href="manuals.html#prime2D" target="new">simple_map2ptcls</a><PRE>
     $ simple_map2ptcls stk=particles_sh.spi stk2=selected_cavgs.spi 
     stk3=cavgs_iter16.spi oritab=prime2D_round_16/prime2Ddoc_16.txt 
     doclist=doclist.txt nthr=8
 </PRE>
 Next we reconstruct a symmetrised volume from the particles using the mapped orientation parameters (called <TT>mapped_ptcls_params.txt</TT> by default) and specifying the symmetry group (<TT>pgrp=d7</TT>)
 <PRE>
     $ simple_eo_recvol stk=particles_sh.spi oritab=mapped_ptcls_params.txt 
     smpd=1.62 msk=60 nthr=8 pgrp=d7
 </PRE>
 After several minutes we obtain a new volume (<TT>recvol_state1msk.spi</TT>) and its resolution
 <PRE>
     &gt;&gt;&gt; RESOLUTION AT FSC=0.143 DETERMINED TO:  12.60
     &gt;&gt;&gt; RESOLUTION AT FSC=0.500 DETERMINED TO:  17.45
 </PRE>
 As <TT>recvol_state1msk.spi</TT> is the default name used internally by <a href="manuals.html#prime3D" target="new">simple_prime3D</a></TT></FONT> it is best to rename it to avoid having it overwritten
 <PRE>
     $ mv recvol_state1msk.spi sym_recvol_state1msk.spi
 </PRE>
 <a href="manuals.html#prime2D" target="new">simple_prime2D</a> and <a href="manuals.html#prime3D" target="new">simple_prime3D</a> also produce a file <TT>fsc_state1.bin</TT> that contains the FSC plot. Make sure to backup this file as  <a href="manuals.html#prime3D" target="new">simple_prime3D</a> will overwrite it if executed in the same folder. Here we just copy it because in the next step we will refine the volume and  <a href="manuals.html#prime3D" target="new">simple_prime3D</a> will require the information contained in <TT>fsc_state1.bin</TT> to initiate the refinement
 <PRE>
     $ cp fsc_state1.bin eo_fsc_state1.bin
 </PRE>
 
 <H3><A NAME="SECTION00074400000000000000">
 Refinement of the symmetrised volume using <a href="manuals.html#prime3D" target="new">simple_prime3D</a></A>
 </H3>

 <P>
 Finally, we refine our initial model while applying D7 symmetry with  <a href="manuals.html#prime3D" target="new">simple_prime3D</a><PRE>
     $ simple_stackops stk=particles_sh.mrc split=8
     $ nohup distr_simple.pl prg=prime3D stk=groel-stk.spi
     vol1=sym_recvol_state1msk.spi smpd=1.62 msk=60 
     eo=yes oritab=mapped_ptcls_params.txt npart=8 &gt; 
     PRIME3DOUT2 &amp;
 </PRE>
 With the first instruction we split the stack for distributed execution. Here the refinement run will be split over 8 different CPU sockets on a Linux cluster (<TT>split=8</TT>; see above for more details). We use <a href="manuals.html#prime3D" target="new">simple_prime3D</a> differently this time. Instead of setting a 20 &#197; low-pass limit, the resolution of the volume is calculated automatically at every iteration (<TT>eo=yes</TT>) starting from our symmetrised volume (<TT>vol1=sym_recvol_state1msk.spi</TT>). After the 10 iterations required for convergence, the final resolution  (better than 8 &#197;) is printed in the end of the <TT>PRIME3DOUT2</TT> output (and also stored in the <TT>fsc_state1.bin</TT>). Examination of the volume shows helical features consistent with the GroEL X-ray structure.
 <PRE>
     &gt;&gt;&gt; RESOLUTION AT FSC=0.143 DETERMINED TO:   7.32
     &gt;&gt;&gt; RESOLUTION AT FSC=0.500 DETERMINED TO:   8.10
 </PRE>
 <B>A note on overfitting:</B> In contrast to most other packages, the only exception being Frealign, SIMPLE does all of its interpolations and correlation calculations in the Fourier domain. Other packages may argue that they do as well but there are subtle important differences. For example, Spider, EMAN and SPARX interpolate polar coordinates in real space and then calculate one-dimensional Fourier transforms along concentric rings to obtain their "polar Fourier transforms", which are in fact polar real images. The advantage of doing it in this way is that you can use a hard mask in real space and avoid including too much background noise in the representation. However, the fundamental disadvantage is that you loose the ability to control which Fourier components are being used for the matching. Initially, it was assumed that the low-pass filtering of the volume based on the FSC was sufficient to avoid overfitting. This has proven to not be true with this kind of representation, as both EMAN2 and SPARX now implement the "gold-standard approach". We instead use gridding interpolation <I>in Fourier space</I> to obtain our polar central sections. The minor disadvantage is that we have to apply a soft-edged mask in real space and risk introducing slightly more background noise in the representation. However, the major advantage is that we can control exactly which Fourier components we use for matching. The default high-pass limit is set to Fourier index 2 but if you think you have a lot of inelastic scattering at low resolution (this is typical for icosahedral viruses) you may want to change the high-pass limit via <TT>hp=X &#197;</TT>. The hard low-pass limit, when <a href="manuals.html#prime3D" target="new">simple_prime3D</a> is executed with <TT>eo=yes</TT> is set according to the <TT>FSC=0.143</TT> criterion. We have yet to detect any overfitting visually or using the <a href="http://www.ncbi.nlm.nih.gov/pubmed/23872039" target="new">noise substitution test</a> with this approach on standard EM data obtained with the underfocusing approach. However, while processing close-to-focus phase plate data we have observed severe overfitting and we recommend to battle it using a hard low-pass limitation with <TT>lpstop=X &#197;</TT>. If you do all your analyses with a hard  low-pass limit of <TT>X</TT> and the resolution extends significantly beyond the limit, there should be no reason to worry about overfitting.
          
          </div>
              </div> 
              
              
              
<!-- END ACCORDION -->
</div>
  
  
  
  
  
  
  
  
  
</div>
<script type="text/javascript">
var Accordion1 = new Spry.Widget.Accordion("Accordion1", { useFixedPanelHeights: false, defaultPanel: -1 });
</script>
<!-- InstanceEndEditable -->
<h1>&nbsp;</h1>
<script type="text/javascript">
var MenuBar1 = new Spry.Widget.MenuBar("MenuBar1", {imgDown:"SpryAssets/SpryMenuBarDownHover.gif", imgRight:"SpryAssets/SpryMenuBarRightHover.gif"});
</script>
</body>
<!-- InstanceEnd --></html>
